---
title: "Building Vector Search from Scratch"
description: "A deep dive into implementing approximate nearest neighbor search, from brute force to HNSW graphs."
date: "2026-02-18"
tags: ["ai", "search", "algorithms"]
published: true
---

Vector search is at the heart of modern AI applications — from semantic search to recommendation systems. In this post, I'll walk through building a vector search engine from scratch.

## Why Vector Search?

Traditional keyword search breaks down when you need to find *semantically similar* content. Vector search solves this by representing data as high-dimensional vectors and finding the nearest neighbors.

```python
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

## The Brute Force Approach

The simplest approach is to compute the distance between the query vector and every vector in the dataset:

```python
def brute_force_search(query, vectors, k=10):
    distances = [
        (i, cosine_similarity(query, v))
        for i, v in enumerate(vectors)
    ]
    distances.sort(key=lambda x: x[1], reverse=True)
    return distances[:k]
```

This works fine for small datasets, but becomes impractical at scale. With 1 million vectors of 768 dimensions, each query requires ~768 million floating point operations.

## Approximate Nearest Neighbors (ANN)

The key insight is that we can trade a small amount of accuracy for massive speed improvements. There are several approaches:

### 1. Locality-Sensitive Hashing (LSH)

LSH uses hash functions that map similar vectors to the same bucket with high probability.

### 2. Product Quantization (PQ)

PQ compresses vectors by splitting them into subvectors and quantizing each independently.

### 3. HNSW (Hierarchical Navigable Small World)

HNSW builds a multi-layer graph where each node is connected to its approximate nearest neighbors. Search starts at the top layer and greedily descends.

```typescript
interface HNSWNode {
  vector: number[];
  connections: Map<number, number[]>; // layer -> connected node ids
}

class HNSW {
  private nodes: HNSWNode[] = [];
  private maxLevel: number = 0;
  private entryPoint: number = 0;

  search(query: number[], k: number): number[] {
    let currentNode = this.entryPoint;

    // Traverse from top layer down
    for (let level = this.maxLevel; level > 0; level--) {
      currentNode = this.greedySearch(query, currentNode, level);
    }

    // Search bottom layer for k nearest
    return this.searchLayer(query, currentNode, k, 0);
  }
}
```

## Benchmarks

At 1M vectors (768d), here's what you can expect:

| Method | QPS | Recall@10 |
|--------|-----|-----------|
| Brute Force | 50 | 100% |
| LSH | 5,000 | 85% |
| PQ | 10,000 | 92% |
| HNSW | 15,000 | 98% |

## Conclusion

Vector search is a fascinating intersection of algorithms, systems engineering, and machine learning. The field is evolving rapidly — with new approaches like DiskANN enabling billion-scale search on a single machine.

The best approach depends on your constraints: dataset size, latency requirements, memory budget, and acceptable recall trade-offs.
